{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to use machine learning to identify the persons of interest (POI) in the Enron scandal.\n",
    "\n",
    "The dataset contains 146 entries (people), 21 features including the POI information (a boolan which indicates if the person if a POI or not).\n",
    "\n",
    "In the dataset, there is 18 people tagged as POI and 128 who are not.\n",
    "\n",
    "We can see in the dataset a lot of feature which a NaN value. It is 44% of NaN value in the whole dataset.\n",
    "\n",
    "The repartition of NaN and Non NaN value is as following;\n",
    "![NaN_NonNaN](images/NaN.png)\n",
    "\n",
    "There is a lot of unknown values. We can notice that the attributes 'loan_advance', 'restricted_stock_value' and 'director_fees contains very few information. There is no NaN Value for the attribute POI.\n",
    "\n",
    "The outliers and invalid values were also checked. The entry TOTAL in the dataset was removed. A check of how the name are composed was done (TOTAL for example is not a valid name). A strange name was found 'THE TRAVEL AGENCY IN THE PARK' which is not a people name. After check this entry has more or less only NaN values. This entry can also be removed.\n",
    "\n",
    "To find the outliers, the z-value was calculed.\n",
    "![z-value](images/z-value.png)\n",
    "This value is more representative than the raw value. We can notice interessing things. First the values are really widly spread. This is probably due to the missing data which are replaced by zero. Secondly we can confirm that the total_payment, the loan_advances and the restricted_loan_defered maximum seem strange but the total_stock_value seems to be like other fields. All values with a z-value more than 4 and less than -2 were checked and no suspect values were found.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three new features were created.\n",
    "- total_benefit (the sum of total_payments and total_stock_value)\n",
    "- fraction_from_poi\n",
    "- fraction_to_poi\n",
    "\n",
    "The total_benefit meta data is there to define the total amount a money earn and not have the payment and the stock values separated. The fraction from/to POI is used to see the amount of mail exchanged to and from POI (a the different POIs organised the fraud together)\n",
    "\n",
    "With the function SelectKBest, the following importance of the features are found:\n",
    "\n",
    "| Feature | Score |\n",
    "|---|---|\n",
    "| exercised_stock_options \t| 25.097542 |\n",
    "| total_stock_value \t| 24.467654 |\n",
    "| bonus \t| 21.060002 |\n",
    "| salary \t| 18.575703 |\n",
    "| total_benefit \t| 17.192568 |\n",
    "| fraction_to_poi \t| 16.641707 |\n",
    "| deferred_income \t| 11.595548 |\n",
    "| long_term_incentive \t| 10.072455 |\n",
    "| restricted_stock \t| 9.346701 |\n",
    "| total_payments \t| 8.873835 |\n",
    "| shared_receipt_with_poi \t| 8.746486 |\n",
    "| loan_advances \t| 7.242730 |\n",
    "| expenses \t| 6.234201 |\n",
    "| from_poi_to_this_person \t| 5.344942 |\n",
    "| other \t| 4.246154 |\n",
    "| fraction_from_poi \t| 3.210762 |\n",
    "| from_this_person_to_poi \t| 2.426508 |\n",
    "| director_fees \t| 2.107656 |\n",
    "| to_messages \t| 1.698824 |\n",
    "| deferral_payments \t| 0.217059 |\n",
    "| from_messages \t| 0.164164 |\n",
    "| restricted_stock_deferred \t| 0.064984 |\n",
    "\n",
    "We see that the features with a score greater than 15 are quite more relevant than the others. Those 5 features will be selected to train the algorithm. The model was also tested with more and less features but the results were less effective. It is a selection of 5 features which is not too few but also not too many. This selection will be kept.\n",
    "\n",
    "For the Decision tree the importance of the features where checked. A rerun was done without the zero importance features. Rerun again by suppressing the less usefull feature. The best score where found with the 5 following features:\n",
    "\n",
    "| Feature | Importance |\n",
    "|---|---|\n",
    "| expenses                 | 0.332150 |\n",
    "| shared_receipt_with_poi  | 0.271719 |\n",
    "| exercised_stock_options  | 0.177438 |\n",
    "| fraction_to_poi          | 0.171020 |\n",
    "| total_benefit            | 0.047673 |\n",
    "\n",
    "We can see that two meta features added (total_benefit and fraction_to_poi) are present in the selection of features. Those meta features improved the performance of the model.\n",
    "\n",
    "For the Gaussian Naive Bayes and the Descision Tree algorihm no scaling is needed. For the KNN, if the features are scaled the results are not better than without. Then no scaling are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm were first checked out the box without any tuning. The default parameters were used.\n",
    "The different algortithm used were:\n",
    "\n",
    "| Algorithm | Accuracy | Precision | Recall | F1 Score | F2 Score | Notes |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Gaussian Naive Bayes | 0.85093 | 0.47409 | 0.39800 | 0.43273 | 0.41120 | the feature with a score greater than 15 were used (Top 6 features)\n",
    "| Decision Tree | 0.83240 | 0.36138 | 0.33500 | 0.34769 | 0.33996 | With the 5 top features selected by importance (see previous question) |\n",
    "| KNN  | 0.87773\t | 0.64360\t| 0.18600\t| 0.28860\t| 0.21683 | the feature with a score greater than 15 were used (Top 6 features) |\n",
    "\n",
    "Without any tuning the algorithm with the best result if the Gausian Naive Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters of an algorithm is to ajust them in order to obtain a better performance. If those parameter are wrong, it can degrade the performance of the learning or can conduct to an overfit.\n",
    "\n",
    "To tune the algorithm the GridSearchCV was used. The Gaussian Naive Bayes Algorithm is not tunable. The KNN and Decision Tree were tuned. Here is the results:\n",
    "\n",
    "| Algorithm | Accuracy | Precision | Recall | F1 Score | F2 Score | Notes |\n",
    "|---|---|---|---|---|---|---|\n",
    "| Gaussian Naive Bayes | 0.85093 | 0.47409 | 0.39800 | 0.43273 | 0.41120 | Not tunable. Given for comparaison |\n",
    "||\n",
    "| KNN (with default paramters) | 0.87773 | 0.64360 | 0.18600 | 0.28860 | 0.21683 | |\n",
    "| KNN (with optimzed parameters) | 0.86773 | 0.69048 | 0.01450 | 0.02840 | 0.01803 | The precision improved but not the accuracy |\n",
    "||\n",
    "| Decision Tree (with default paramters) | 0.82980 | 0.35206 | 0.32900 | 0.34014 | 0.33337 | |\n",
    "| Decision Tree (with optimzed parameters) | 0.88667 | 0.58532 | 0.51450 | 0.54763 | 0.52726 | All metrics improved |\n",
    "\n",
    "The best results are achieved with the Decision Tree algorithm with a selection of the features according of the importance and the following parameters:\n",
    "\n",
    "| Parameter | Value |\n",
    "|--|--|\n",
    "| splitter | 'best' |\n",
    "| max_leaf_nodes | 5 |\n",
    "| min_samples_leaf | 4 |\n",
    "| random_state | 42 |\n",
    "| criterion | 'entropy'\n",
    "| min_samples_split | 2 |\n",
    "| max_depth | 2 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation is used to check the performance of the model. The model is trained on a part of the dataset et validated with another part it didn't see before. This phase is important to be sure that the model will be able to deliver reliable output with some new data. \n",
    "\n",
    "If it is wrongly done (e.g train the model on the whole dataset) there is a risk of overfit. The result will be good with the training set but very bad with some real cases.\n",
    "\n",
    "Another classic mistake is to split the data in a non representative way. When the traning set does not contains enougth type of a certain labeled data. \n",
    "\n",
    "In our Enron example, there is only 18 POI which is very low. It is quite hard to split the dataset in an effective manner. That why here the StratifiedShuffleSplit function is used in order to imporve the repartition of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision is the ratio between true positive and all positiv results. In other words it is the ability of the model to find positive results (true or false positive).\n",
    "\n",
    "The recall is the ratio between true positive and the sum of true positiv and false negative. In other words it is the ability of the model to find true positive results.\n",
    "\n",
    "In our case here it is not critical if we have some false positive because we want to find POI. It is important to detect a real POI as a POI and the false positive will be some false alert.\n",
    "\n",
    "The more relevant parameter between precision and recall is the recall in our case. The model should improve this score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly the Decision tree algotrithm was choosen. We got the best scores. THe relevant features were selected according to their importances and the parameters were tuned with a grid search algorithm.\n",
    "\n",
    "Here is the final results:\n",
    "\n",
    "| Algorithm | Accuracy | Precision | Recall | F1 Score | F2 Score\n",
    "|---|---|---|---|---|---|\n",
    "| Decision Tree (with optimzed parameters) | 0.88667 | 0.58532 | 0.51450 | 0.54763 | 0.52726 |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
